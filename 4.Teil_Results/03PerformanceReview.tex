\chapter{Computational Performance}



\section{Time Complexity of Gradient Computation}

Figure \ref{fig:gradient_time_complexity} shows the time complexity of the gradient computation for AD and SYM.
The dotted line indicates a time complexity of $\mathcal{O}(N)$, where $N$ is the number of voxels.
Each voxel had, as mentioned, six optimisation parameters.
All tests were approximately parallel to the dotted line, which indeed indicates a time complexity of $\mathcal{O}(N)$ for both implementations.
Note that \emph{AD laptop} is tested on another device than the rest.
Regarding \emph{AD CPU/GPU}, the CPU and GPU automatic gradient calculations were merged, since the performance did not change as a result of choice of device. % as the result was the same.
Moreover, it is important to notice that the speed-up of the parallelisation was not proportional to the number of cores, which was 12.
%RSD: Update with SYM as well? Manually. Not the most important thing. 
%RSD: Mentioned Laptop vs Server. Note that GPU and CPU are equally fast. However, if algorithm extended so that the amount of transfer of data is at a relative minimum, a significant speed-up will occur. 
\begin{figure}[h!]
    \centering
    \includesvg[width=1\textwidth]{../XRD_CT/Plotting/thesis_plots/gradient_computation.svg}
    \caption[Time Complexity of Gradient Computation]{ The gradient computation time of AD and SYM for different number of voxels using a laptop, GPU, MEX-functions, and parallelisation, respectively. % Should test CPU on Gamma instead. Mind myself. Not the most important thing. Change text 
        The time complexity was about $\mathcal{O}(N)$ for all methods, where N is the number of voxels.}
    \label{fig:gradient_time_complexity}
\end{figure}

\clearpage
\section{Convergence of SAXSTT}

Figure \ref{fig:Loss_curve_optimal} shows the convergence of SAXSTT after \num{100} iterations for the different implementations.
Specifically, reconstructions of the carbon knot and the Phantom data set are included.
The characteristic convergence of the Spherical Harmonics method is evident.
Since the different sets of parameters were optimised independently before the final optimisation, several exponential decays are visible.
Each exponential decay corresponds to each of the top boxes in Figure \ref{fig:flowchart_SAXSTT} and \ref{fig:flowchart_ADSAXSTT}.
The same step-wise reconstruction was not implemented for EXPSIN, which therefore experienced an initial exponential decay followed by an almost flat convergence.
In fact, only $\num{10}\%$ of the EXPSIN iterations showed exponential decay; the rest were almost flat.
However, EXPSIN showed a notable second decay after ca. \num{55} iterations, and is seemingly still decaying at the end of the \num{100} iterations, as opposed to the other methods.
Another fascinating observation was that the loss of the stepwise optimisation drastically increased after \num{30} iterations, meaning the first iteration of the anisotropic coefficients optimisation.
Moreover, the loss of the carbon knot reconstruction was only halved after \num{100} iterations, whereas the loss of the Phantom data set was reduced by a factor of \num{10}.
In terms of computational time, the symbolic reconstruction of the Phantom data set was significantly faster than the rest, since only \num{50} of the \num{100} iterations were performed.
Apart from that, the "AD"-algorithm and the "EXPSIN"-algorithm were within twice the time of the "SYM"-algorithm.


%Update with EXPSIN as well? Evt for carbon knot. Evt include all runs or do normalisation. Difficult to know right now. 

\begin{figure}[h!]
    \centering
    \includesvg[width=1\textwidth]{../XRD_CT/Plotting/thesis_plots/Loss_curve_optimal.svg} %RSD: Remember inkscape formatter and scaling. More changes needed to the plot. Generalise number og lines. 
    \caption[Loss Curves of Reconstructions]{ The loss curves of the respective performed reconstructions. Each curve was normalised to the initial loss.
        CK is short for carbon knot, and P stands for Phantom. }
    \label{fig:Loss_curve_optimal}
\end{figure}