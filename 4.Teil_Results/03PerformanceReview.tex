\chapter{Computation Performance}


%RSD: Does not seem to be necessary
% \section{Symbolic SAXSTT}

% \section{Automatic SAXSTT in MATLAB}

% \section{Optimised Automatic SAXSTT in Python}

\section{SAXSTT Time Complexity}

% Figure \ref{fig:convergence_time_complexity} shows the time complexity of SAXSTT for the different implementations when the initial conditions are close to the optimal solution.

% \begin{figure}[h!]
%     \centering
%     \includesvg[width=1\textwidth]{../XRD_CT/Plotting/thesis_plots/SH_performance_curves.svg} %RSD: Update despite the new being ugly. Or exclude. This would require to do test one more time with fixed number of iterations.  
%     \caption{ f   }
%     \label{fig:convergence_time_complexity}
% \end{figure}

Figure \ref{fig:gradient_time_complexity} shows the time complexity of the gradient computation for the different implementations. %RSD: Update with SYM as well? Manually. Not the most important thing. 
Trying to render labels.
\begin{figure}[h!]
    \centering
    \includesvg[width=1\textwidth]{../XRD_CT/Plotting/thesis_plots/gradient_computation.svg}
    \caption{ The gradient computation time for different number of voxels using a CPU and a GPU, respectively. % Should test CPU on Gamma instead. Mind myself. Not the most important thing. 
        The time complexity is about $O(N)$ for both methods, where N is the number of voxels. However, the GPU provides approximately an 8-fold speedup when the number of voxels is sufficiently high.}
    \label{fig:gradient_time_complexity}
\end{figure}

\section{Convergence of SAXSTT}

Figure \ref{fig:Loss_curve_optimal} shows the convergence of SAXSTT after 100 iterations for the different implementations when the initial conditions are close to the optimal solution. %Update with EXPSIN as well? Evt for carbon knot. 

\begin{figure}[h!]
    \centering
    \includesvg[width=0.6\textwidth]{../XRD_CT/Plotting/thesis_plots/Loss_curve_optimal.svg} %RSD: Remember inkscape formatter and scaling. More changes needed to the plot. Generalise number og lines. 
    \caption{ The convergence of the CGD algorithm is steep for each of the optimisation steps.  }
    \label{fig:Loss_curve_optimal}
\end{figure}

Figure \ref{fig:Loss_curve_worst} shows the convergence of SAXSTT for the different implementations when the initial conditions are far from the optimal solution.

\begin{figure}[h!]
    \centering
    %\includesvg[width=0.6\textwidth]{../XRD_CT/Plotting/thesis_plots/Loss_curve_optimal.svg} %RSD: Remember inkscape formatter and scaling. More changes needed to the plot. Generalise number og lines. 
    \caption{ The CGD algorithm is prone to getting stuck in local minima, especially when the initial conditions are not sufficiently good guesses.  }
    \label{fig:Loss_curve_worst}
\end{figure}

Figure \ref{fig:Loss_curve_carbon}

\begin{figure}[h!]
    \centering
    \includesvg[width=1\textwidth]{../XRD_CT/Plotting/thesis_plots/Loss_curve_carbon.svg} %RSD: Remember inkscape formatter and scaling. More changes needed to the plot. Generalise number og lines. 
    \caption{ The CGD algorithm is prone to getting stuck in local minima, especially when the initial conditions are not sufficiently good guesses.  }
    \label{fig:Loss_curve_carbon}
\end{figure}