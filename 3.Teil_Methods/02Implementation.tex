
\chapter{Implementation}

\section{Proof of Concept Automatic Differentiation}\label{sec:proof_of_concept_AD}

The first checkpoint in implementing automatic differentiation for the SAXSTT routine, was to witness an expected behaviour from the AD engines of choice.
A range of different engines were tested using the cost functions listed in Table \ref{} with random initial values.

\begin{table}[h]
    \centering
    \caption{}
    \label{tab:cost_functions}
    \begin{tabular}{ c c }
        \hline
        \textbf{Cost Function} & \textbf{Equation}                \\
        \hline
        f(x,y)                 & $\ln{x} + xy$                    \\
        g(x,y)                 & $\ln(xy)\sin(y)$                 \\
        h(x)                   & $64x(1-x) (1-2x)^2(1-8x+8x^2)^2$ \\
    \end{tabular}
\end{table}
% RSD: Is this useful?



The automatically calculated gradients were compared to symbolically derived gradients and a numerical central differences.
Hence, the accuracy and functionality of the engines were confirmed before the proper implementation.
Moreover, the performance of the engines were compared relative to the symbolic function evaluation and the numerical gradients.
As all engines showed expected behaviour when computing gradients,
MATLAB's deep learning library was chosen as preferred engine since the SAXSTT routine was written in MATLAB.
This allowed for much reuse of original code, and thus a swift implementation.
However, it was also noted that Pytorch's AD engine was considerably faster than MATLAB's engine, and that the Pytorch pool of supported built-in functions was much larger.
Eventually, Pytorch was thus later utilised in an effort to optimise the AD SAXSTT routine, which will be covered in Section \ref{sec:AD_pytorch}.


\section{Optimisation Using Symbolic Gradients}
The gradients of the original SAXSTT routine were derived symbolically and implemented in MATLAB by Liebi et. al \cite{liebi2015nanostructure}.
This routine served as the most important tool of validation when implementing AD for SAXSTT,
as the AD implementation is simply a variation of the original code in such a way that the AD engine only encounters supported functionality.
As a result, the SAXSTT routine was thoroughly analysed and categorised into a hierarchy of optimisation routines. % Ref to theory CGD and SAXSTT?
The topmost part of the hierarchy is the optimisation of the individual spherical harmonics parameters.
Each parameter was adjusted using a conjugate gradient descent algorithm,
which consisted of a gradient calculation, determination of direction of optimisation, a linesearch, and an update of the parameter.

The gradient calculation consisted of an estimation of the scattering intensity in all directions, a comparison to the experimental scattering data,
and a calculation of the gradient with respect to the optimised parameter.
The first part of the scattering intensity estimation was to create a tomogram of the sample given the current set of parameters.
Then, bilinear interpolation was applied to assign the scattering from each voxel to a set of projection pixels.
Moreover, the resulting projection data was smoothed using a Gaussian filter to reduce bilinear interpolation artefacts.
Bilinear interpolation was applied once again to assign one gradient value in each voxel of the tomogram. % Shortly summarised. Ref Figures. Flowcharts are nice. 

With the obtained gradients, a line search was performed iteratively in the direction determined by the CGD algorithm, which was explained in Section. %\ref{sec:theory_CGD}.
Further details of the line search is not of importance for the AD implementation, because it is not directly related to the gradient calculations.
A complete flowchart of the symbolic gradient routine, which also serves as a general idea of the SAXSTT routine, is shown in Figure \ref{fig:flowchart_SAXSTT}.

%RSD: Decide on a flowchart style. 
\begin{figure}
    \centering
    %\includesvg{../XRD_CT/Figures/Flowchart_optimisation_algorithm.svg}
    %\includegraphics[width=\textwidth]{../XRD_CT/Figures/Flowchart_SAXSTT.pdf} %Here, the font is scaled, but have enough margins. 
    \includegraphics[width=0.48\textwidth]{../XRD_CT/Figures/Flowchart_optimisation_algorithm.pdf}
    \includegraphics[width=0.48\textwidth]{../XRD_CT/Figures/Flowchart_cgmin.pdf}
    %\includegraphics[width=\textwidth]{../XRD_CT/Figures/Flowchart_symbolic_gradient.pdf}
    \caption{Flowchart of the SAXSTT routine.}
    \label{fig:flowchart_optimisation}
\end{figure}

\begin{figure}
    \centering
    \includegraphics[width=\textwidth]{../XRD_CT/Figures/Flowchart_symbolic_gradient.pdf}
    \caption{Flowchart of the SAXSTT routine.}
    \label{fig:flowchart_gradients}
\end{figure}

\begin{figure}
    \centering
    %\includesvg{../XRD_CT/Figures/Flowchart_optimisation_algorithm.svg}
    \includegraphics[width=\textwidth]{../XRD_CT/Figures/Flowchart_SAXSTT.pdf} %Here, the font is scaled, but have enough margins. 
    \caption{Flowchart of the SAXSTT routine.}
    \label{fig:flowchart_SAXSTT}
\end{figure}

\section{Automatic Differentiation in MATLAB}%?
\label{sec:AD_matlab}
As mentioned,
a working AD SAXSTT routine was implemented into the already existing MATLAB SAXSTT routine
through modifications that were compatible to MATLAB's deep learning library.
The essential aspects from this library were a special array called \emph{dlarray},
\emph{dlfeval} special function evaluation, and automatic gradient calculation \emph{dlgradient}.
Firstly, the \emph{dlarray} is an array class that was used to initialise the differentiation variables.
An underlying computational graph is created from operations performed on the array within the cost function evaluation.
The \emph{dlfeval} function is used to evaluate the cost function given the input of differentiation variables.
Finally, the \emph{dlgradient} function is called at the end of the cost function in order to execute automatic differentiation.
As mentioned in Section \ref{sec:theory_AD}, the chain rule, Equation \eqref{eq:chain_rule}, is applied to the stored computational graph in a backpropagation algorithm.

This basic idea was implemented first for the optimisation of the spherical harmonics coefficients.
Here, the computational graph was shorter than for orientation optimisation, since the orientation was considered to be constants.
Thus, all the computations prior to the introduction of the spherical harmonics coefficients were performed outside the \emph{dlfeval} cost function evaluation.
Nevertheless, several pieces of code had to be modified in order to make the routine compatible with the AD library.
Most importantly, the original MEX-function routines implemented to efficiently perform page-multiplication and bilinear interpolation, the bottlenecks of the SAXSTT routine, had to be rewritten in MATLAB.
The MATLAB implementation was not optimised by altering the structure of the code, since the goal was to create a near identical SAXSTT routine with automatic differentiation capabilities.
However, page-multiplication kept its efficieny with the built-in function \emph{pagemtimes}.
On the other hand, bilinear interpolation was performed using a double for-loop, like the original MEX-function implementation, at the cost of a considerable speed decrease.
Finally, the convolution function \emph{dlconv} applied the mentioned Gaussian smoothing of the bilinear interpolation artefacts. % Ref to Figure \ref{fig:flowchart_SAXSTT}. 
The error evaluation was kept identical to the original SAXSTT routine, but was at the same time integrated side by side with the gradient calculation using if-statements.

Consequently, the remaining part of the AD SAXSTT routine was the optimisation of the orientation as well as the full optimisation.
In this case, the differential variables were used early in the cost function evaluation, and the computational graph was longer.
Moreover, one additional function, which calculated the spherical harmonics polynomials, had to be rewritten using a short for-loop instead of MATLAB built-in functions.

Finally, the \emph{dlarrays} were also initialised to \emph{gpuArrays} in order to possibly speed up calculations on servers with powerful GPUs.


\section{Automatic Differentiation Using Pytorch}\label{sec:AD_pytorch}

In the initial testing of automatic differentiation, it was noted that the AD engine by Pytorch was powerful and versatile.
Therefore, implemented parts of the AD routine in MATLAB were translated to Python, and optimised using Pytorch.
The code structure was kept the same, but the implementation was modified to be compatible with Pytorch and CUDA GPU computing.
Moreover, the projection orientations were integrated to a single batch rather than utilising parallel for-loops. %RSD: Has to be done. 
The batch was an essential optimisation choice, as it minimised the communication frequency between the CPU and GPU, and between Python and MATLAB.
The other essential optimisation was bilinear interpolation performed using array operations on multidimensional arrays.
In contrast, the MATLAB implementation used double for-loops of single elements.
At the same time, different data types, different reshaping order conventions, and issues concerning compatible environments had to be considered.
The forward and backward passes were initialised and finalised using a series of data type conversions.
Moreover, an efficient F-order reshaping function was found online.
Unfortunately, MATLAB and Python had library inconsistencies on the server,
and the Python function had to be run in a separate process, which limited performance and the memory transfer capacity.

\section{Implementation of Alternative Cost Function}\label{sec:alternative_cost_function}

The cost function used in the SAXSTT routine is a comparison of the experimental and estimated SAXS projection intensities.
Originally, the estimated intensity was the absolute value squared of a linear combination of spherical harmonics, as shown in Equation \eqref{eq:reciprocal_space_map}.
However, it was desired to express the atomic form factor using the expression listed in Equation \eqref{eq:exp_sin_squared}.
This implementation required some modifications of the SAXSTT routine, mainly initialising new variables and change the form factor computation in the cost function evaluation.
It is important to notice that no gradients had to be derived by hand.
A batch approach in Pytorch was the most optimised code structure, and was chosen for this task.

