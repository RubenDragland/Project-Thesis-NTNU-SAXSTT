
\chapter{Implementation}

\section{Proof of Concept Automatic Differentiation}\label{sec:proof_of_concept_AD}

The first checkpoint in implementing automatic differentiation for SAXSTT was to witness an expected behaviour from the AD engines of choice.
A range of different engines were tested on a set of simple functions with random initial values.

The automatically calculated gradients were compared to symbolically derived gradients and numerical central differences.
Hence, the accuracy and functionality of the engines were confirmed before moving on with the proper implementation.
Moreover, the performance of the engines was compared relative to the symbolic function evaluation and the numerical gradients.
As all engines showed expected behaviour when computing gradients,
MATLAB's deep learning library was initially chosen as the preferred engine since the original SAXSTT algorithm was written in MATLAB.
This allowed for much reuse of already written code, and thus a swift first implementation.
However, it was also noted that Pytorch's AD engine was considerably faster, and that the Pytorch pool of supported built-in functions was much larger.
Eventually, Pytorch was thus later utilised in an effort to optimise the AD SAXSTT algorithm, which will be covered in Section \ref{sec:AD_pytorch}.


\section{Understanding the SAXSTT Algorithm}\label{sec:understanding_SAXSTT}
The gradients of the original SAXSTT algorithm were derived symbolically and implemented in MATLAB by Liebi et. al \cite{liebi2015nanostructure}.
The results of this algorithm served as the most important tool of validation when implementing AD for SAXSTT,
as the AD implementation is a variation of the original code in such a way that the AD engine only encounters supported functionality.
As a result, the symbolic SAXSTT routine was thoroughly analysed and categorised into a hierarchy of optimisation routines. % Ref to theory CGD and SAXSTT?
The topmost part of the hierarchy is the optimisation of the individual spherical harmonics parameters.
Each parameter was adjusted using CGD,
which consisted of a gradient calculation, determination of direction of optimisation, a linesearch, and an update of the respective parameters.

The gradient calculation consisted of an estimation of the scattering projection intensities, a comparison to the experimental scattering data,
and a calculation of the gradient with respect to the optimised parameters.
The first part of the scattering intensity estimation was to create a tomogram of the sample given the current set of parameters.
Then, bilinear interpolation was applied to assign the scattering from each voxel to a set of projection pixels.
Moreover, the resulting projection data was smoothed using a Gaussian filter to reduce bilinear interpolation artefacts.
Bilinear interpolation was applied once again to assign one gradient value in each voxel of the tomogram. % Shortly summarised. Ref Figures. Flowcharts are nice. 

With the obtained gradients, a line search was performed iteratively in the direction determined by the CGD algorithm, as explained in Section \ref{sec:theory_CGD}.
Further details of the line search is not of importance for the AD implementation, because it is not directly related to the gradient calculations.
A complete flowchart of the symbolic gradient routine, which also serves as a general idea of the reconstruction part of SAXSTT, is shown in Figure \ref{fig:flowchart_SAXSTT}.

% %RSD: Decide on a flowchart style. 
% \begin{figure}
%     \centering
%     %\includesvg{../XRD_CT/Figures/Flowchart_optimisation_algorithm.svg}
%     %\includegraphics[width=\textwidth]{../XRD_CT/Figures/Flowchart_SAXSTT.pdf} %Here, the font is scaled, but have enough margins. 
%     \includegraphics[width=0.48\textwidth]{../XRD_CT/Figures/Flowchart_optimisation_algorithm.pdf}
%     \includegraphics[width=0.48\textwidth]{../XRD_CT/Figures/Flowchart_cgmin.pdf}
%     %\includegraphics[width=\textwidth]{../XRD_CT/Figures/Flowchart_symbolic_gradient.pdf}
%     \caption{Flowchart of the SAXSTT routine.}
%     \label{fig:flowchart_optimisation}
% \end{figure}

% \begin{figure}
%     \centering
%     \includegraphics[width=\textwidth]{../XRD_CT/Figures/Flowchart_symbolic_gradient.pdf}
%     \caption{Flowchart of the SAXSTT routine.}
%     \label{fig:flowchart_gradients}
% \end{figure}

\begin{figure}
    \centering
    %\includesvg{../XRD_CT/Figures/Flowchart_optimisation_algorithm.svg}
    \includegraphics[width=\textwidth]{../XRD_CT/Figures/Flowchart_SAXSTT_optimised.pdf} %Here, the font is scaled, but have enough margins. 
    \caption[Flowchart of Symbolic SAXSTT]{Flowchart of symbolic SAXSTT.
        For each of the optimisation blocks, a for-loop of CGD steps is performed.
        Each CGD step starts with a gradient calculation.
        The gradient is used to determine the direction of optimisation, which is orthogonal to previous steps.
        A line search is performed in the direction of optimisation to find the optimal step size.
        With the optimal step size, the parameters are updated, and the next CGD step is performed.
    }
    \label{fig:flowchart_SAXSTT}
\end{figure}

\section{Automatic Differentiation in MATLAB}%?
\label{sec:AD_matlab}
As mentioned,
a working AD SAXSTT algorithm was implemented  in MATLAB
through modifications that were compatible with MATLAB's deep learning library.
The essential aspects from this library were a special array called \emph{dlarray}, special function evaluation \emph{dlfeval}, and automatic gradient calculation \emph{dlgradient}.
Firstly, the \emph{dlarray} is an array class that was used to initialise the differentiation variables.
An underlying computational graph is created from operations performed on the array within the cost function evaluation.
The \emph{dlfeval} function is used to evaluate the cost function given the input of differentiation variables.
Finally, the \emph{dlgradient} function is called by the cost function in order to execute automatic differentiation.
As mentioned in Section \ref{sec:theory_AD}, the chain rule, Equation \eqref{eq:chain_rule}, is applied to the stored computational graph in a backpropagation algorithm.

% This basic idea was implemented first for the optimisation of the spherical harmonics coefficients.
% Here, the computational graph was shorter than for orientation optimisation, since the orientation was considered to be constants.
% Thus, all the computations prior to the introduction of the spherical harmonics coefficients were performed outside the \emph{dlfeval} cost function evaluation.
% Nevertheless, several pieces of code had to be modified in order to make the routine compatible with the AD library.

The resulting AD SAXSTT implementation ended up having some significant disadvantages.
Most importantly, the original MEX-function routines implemented to efficiently perform page-multiplication and bilinear interpolation, the bottlenecks of the SAXSTT routine, had to be written using MATLAB.
This is because MEX-functions are written in C, which is not compatible with the AD library.
The MATLAB implementation was not optimised by altering the structure of the code, since the goal was to create a near identical SAXSTT routine with automatic differentiation capabilities.
However, page-multiplication kept its efficieny with the built-in function \emph{pagemtimes}.
On the other hand, bilinear interpolation was performed using a double for-loop, like the original MEX-function implementation, at the cost of a considerable speed decrease.
Finally, the convolution function \emph{dlconv} applied the mentioned Gaussian smoothing of the bilinear interpolation artefacts.
% Ref to Figure \ref{fig:flowchart_SAXSTT}. for AD?


\section{Automatic Differentiation Using Pytorch}\label{sec:AD_pytorch}

In the initial testing of automatic differentiation, it was noted that the AD engine by Pytorch was powerful and versatile.
Therefore, the automatic gradient calculation was translated to Python, and optimised using Pytorch.
The essence of the process described in Section \ref{sec:AD_matlab} was repeated,
meaning that the implementation was modified to be compatible with Pytorch and CUDA GPU computing.
Moreover, the projection orientations were integrated to a single batch rather than utilising parallel for-loops.
The batch was an essential optimisation choice, as it minimised the communication frequency between the CPU and GPU, and between Python and MATLAB.
The other essential optimisation was bilinear interpolation performed using array operations on multidimensional arrays.
In contrast, the MATLAB implementation used double for-loops of single elements.
At the same time, different data types, different reshaping order conventions, and issues concerning compatible environments between MATLAB and Python had to be considered.
The forward and backward passes were initialised and finalised using a series of data type conversions.
Moreover, an efficient F-order reshaping function was found online, ensured that performed operations were identical in MATLAB and Python in terms of reshaping.

\section{Implementation of Alternative Cost Function}\label{sec:alternative_cost_function}

The cost function used in the SAXSTT routine is a comparison of the experimental and estimated SAXS projection intensities.
Originally, the estimated intensity was the absolute value squared of a linear combination of spherical harmonics, as shown in Equation \eqref{eq:reciprocal_space_map}.
However, it was desired to express the reciprocal space map using the expression listed in Equation \eqref{eq:exp_sin_squared}.
%RSD: Plots of DoA and 1D shape? Have already in theory...

\noindent
This implementation required some modifications of the AD SAXSTT algorithm, mainly initialising new variables and changing the form factor computation in the cost function evaluation.
It is important to notice that no gradients had to be derived by hand.
A batch approach in Pytorch was the most optimised code structure, and was chosen for this task.

Finally, the versatility of automatic differentiation was utilised to develop an optimised version of the alternative cost function.
Note that the SAXSTT algorithm that utilised symbolically calculated gradients will be referred to as \emph{SYM} or \emph{the "SYM"-algorithm},
while the AD version will be referred to as \emph{AD}. Reconstructions performed using the alternative cost function will be denoted \emph{EXPSIN}.


%RSD: Move to results
% Additionally, a refinement of the proposed alternative cost function was implemented.
% In order to keep the different components independent, the exponential term, which controlled the uniaxiality of the reciprocal space map, had to be normalised.
% The normalisation was performed by dividing the exponential term by its integral from $0$ to $\pi$.
% Due to the lack of built-in numerical integration functions in Pytorch,
% vectorised numerical integration using the simpsons rule was implemented.
% The number of integration points was chosen to be $N = 500$ to keep the computational burden low.
% In addition, this number of points proved to be as effective or better than higher number of points,
% something which was uncovered by testing the integration when the exponential term was equal to 1.
% The integrator returned a value in the order of $10^{-7}$, where the true value should be 0.
% Finally, the computed normalisation factor was ensured to be positive, since the integrator had a tendency to return negative values during the mentioned test.


As a final note, the flowchart of the SAXSTT algorithm for when the gradients are automatically calculated is shown in Figure \ref{fig:flowchart_ADSAXSTT}.
The essential distinction is that all the operations after the evaluation of the cost function are built in to the AD engine, and will be updated automatically
based on the operations performed in the cost function evaluation.

\begin{figure}
    \centering
    \includegraphics[width=\textwidth]{../XRD_CT/Figures/Flowchart_ADSAXSTT_optimised.pdf}
    \caption[Flowchart of AD SAXSTT]{Flowchart of the SAXSTT algorithm when the gradients are automatically calculated.
        For each of the optimisation blocks, a for-loop of CGD steps is performed.
        Each CGD step starts with a gradient calculation.
        The gradient is used to determine the direction of optimisation, which is orthogonal to previous steps.
        A line search is performed in the direction of optimisation to find the optimal step size.
        With the optimal step size, the parameters are updated, and the next CGD step is performed.
        The designated AD engine will automatically calculate the gradient of the cost function based on the operations performed in the evaluation.
    }
    \label{fig:flowchart_ADSAXSTT}
\end{figure}