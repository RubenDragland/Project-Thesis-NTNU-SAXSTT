\chapter{Machine Learning Optimisation}

\section{Maximum Likelihood Estimation}

The maximum likelihood estimator is defined to be the set of parameters $\bm{\theta}_{ML}$ that maximise the probability $P(\bm{y}\left | \right \bm{x}; \bm{\theta})$. In other words, it chooses the parameters that produce the most probable estimation $\bm{\hat{y}}$ of the true output $\bm{y}$ given the input $\bm{x}$. Equation \eqref{eq:Max_likelihood} defines this mathematically \cite{Goodfellow-et-al-2016}.

\begin{equation}\label{eq:Max_likelihood}
     \bm{\theta}_{ML} = \argmax_{\bm{\theta}} P(\bm{y}\left | \right \bm{x}; \bm{\theta})
\end{equation}

ML estimation is an example of supervised learning, because the true output, the targets are known. In supervised learning, the estimation is evaluated by computing the error relative to the true output. The expression for the total error of the model is called the cost function $J(\bm{\theta})$, which is a sum of loss functions $\mathcal{L}(\bm{y},\bm{\hat{y}} )$ representing the error of a single data point. 

\section{Gradient Descent}

Gradient descent is an optimisation algorithm that updates the model's parameters based on the gradient of the cost function and the step size $\alpha$, as shown in Equation \eqref{eq:GD} \cite{ruder2016overview}.

\begin{equation}\label{eq:GD}
    \bm{\theta} = \bm{\theta} - \alpha \bm{\nabla_{\theta}} J(\bm{\theta})
\end{equation}

The idea of this algorithm is that by following the gradient of the cost function, it is possible to find the global, or at least a sufficiently good local, minima in parameter space. In this way, the estimation will converge and minimise the error. Gradient descent in parameter space is visualised in Figure \ref{fig:GD_surface} 
% Matplotlib surface plot with path. + Make gif. 

\begin{figure}
    \centering
    \includegraphics{}
    \caption{A surface plot of the space of error for a model. The trajectory shows the path of parameter optimisation based on gradient descent}
    \label{fig:GD_surface}
\end{figure}

\section{Conjugate Gradient Descent}

%There are several optimisation algorithms that are based on GD, for instance stochastic gradient descent (SGD) and conjugate gradient descent (CGD). The latter is shortly summarised composed of determination of steepest descent, a line search to find an appropiate step size $\alpha$, and the completion of a step. 

The basic gradient descent algorithm is prone to require many iterations before converging. When solving computationally expensive optimisation tasks using gradient descent, one should therefore consider to use a variant such as Conjugate Gradient Descent (CGD). This method serves as a compromise between basic first order gradient descent, which is also referred to as "steepest descent", and Newton's second order method. The latter uses the Hessian to converge in a small number of highly expensive iterations.

%Consider illustrating the Hessian and/or the Jacobian

% Some equations regarding CGD... Do not now it that well.


\section{Automatic Differentiation}

