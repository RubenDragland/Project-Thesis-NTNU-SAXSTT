\chapter{Machine Learning Optimisation}
\label{ch:optimisation}

\section{Maximum Likelihood Estimation}


The maximum likelihood estimator is defined to be the set of parameters $\bm{\theta}_{ML}$ that maximise the probability $P(\bm{y} \mid \bm{x}; \bm{\theta})$.
In other words, it chooses the parameters that produce the most probable estimation $\bm{\widehat{y}}$ of the true output $\bm{y}$ given the input $\bm{x}$.
Mathematically, the maximum likelihood estimator is defined as

\begin{equation}\label{eq:Max_likelihood}
    \bm{\theta}_{ML} = \argmax_{\bm{\theta}} P(\bm{y} \mid \bm{x}; \bm{\theta}),
\end{equation}
\noindent
where $\argmax_{\bm{\theta}}$ returns the arguments that maximise the probability.

ML estimation is an example of supervised learning, because the true output, called the targets, are known.
In supervised learning, the estimation is evaluated by computing the error relative to the true output.
The expression for the total error of the model is called the cost function $J(\bm{\theta})$,
which is a sum of loss functions $\mathcal{L}(\bm{y},\bm{\widehat{y}} )$ representing the error of a single data point.

\section{Gradient Descent}

Gradient descent is an optimisation algorithm that updates the model's parameters based on the gradient of the cost function and the step size $\alpha$ \cite{ruder2016overview}.
After each backward pass, the parameters of the model are updated according to the following equation:

\begin{equation}\label{eq:GD}
    \bm{\theta} = \bm{\theta} - \alpha \bm{\nabla_{\theta}} J(\bm{\theta})
\end{equation}

The idea of this algorithm is that by following the gradient of the cost function,
the estimation will converge to the correct solution of the problem,
which naturally results in the minimal error.
Gradient descent in parameter space is visualised in Figure \ref{fig:GD_surface}
% Matplotlib surface plot with path. + Make gif. 

\begin{figure}
    \centering
    %\includegraphics{}
    \includesvg[width=1\textwidth]{../XRD_CT/Plotting/plots/GD_new.svg}
    \caption{A surface plot of the space of error for a model. The trajectory shows the path of parameter optimisation based on gradient descent.
        Notice that the smooth curve is an indication of many steps with a small step size. The animation belonging to the plot is available at % Github hyperref. \url{ }
    }
    \label{fig:GD_surface}
\end{figure}

\begin{figure}
    \centering
    \includesvg[width = 1\textwidth]{../XRD_CT/Plotting/plots/CGD_exact.svg}
    \caption{A surface plot of the space of error for a model. The trajectory shows the path of parameter optimisation based on conjugate gradient descent.
        Notice that the curve consists of a few, but long, steps. The animation belonging to the plot is available at % Github hyperref. \url{ }
    }
    \label{fig:CGD_surface}
\end{figure}


%Notes
%Order of the CGD
%Krylov subspace
%Correct Algorithm
%Elaborate AD and Vanishing Gradients. 

\section{Conjugate Gradient Descent}\label{sec:theory_CGD}
\noindent
The basic gradient descent algorithm is prone to require many iterations before converging.
When solving computationally expensive optimisation tasks,
one should therefore consider a method like Conjugate Gradient Descent (CGD).
This method serves as a compromise between basic first order gradient descent and Newton's second order method.
Unlike the basic gradient descent algorithm, CGD includes additional computations in order to reduce the number of iterations required to converge.
At the same time, CGD is still a first order method, as these additional computations do not include calculating the Hessian, the second derivatives.
Instead, additional computations evolve around assessing the potential error in the next step, rather than calculating additional gradients.
Hence, CGD is computationally less expensive than Newton's method, since
the latter uses the Hessian to converge in a small number of highly expensive iterations.
The essential characteristics of the CGD algorithm will be summarised in this section, while it is clearly explained by Jonathan Richard Shewchuk \cite{shewchuk1994introduction}.

The first improvement over basic gradient descent is a line search to determine the optimal step size $\alpha$ in the direction of the calculated gradients.
The optimal step size is determined by computing the cost function $J(\bm{\theta})$ for a range of step sizes $\alpha$.
At the minima, the gradient vector of the current and next iteration are orthogonal, which optimises the path of convergence.  %minimises the directional derivative of the cost function.
Intuitively, this reduces the number of gradient calculations required to reach convergence, as the algorithm always exhausts the potential of the current direction, and proceeds orthogonal to this direction.

However, with this procedure, which is commonly referred to as "Method of Steepest Descent", the algorithm often steps in the same direction multiple times.
To prevent this, CGD only performs one step per basis vector in a set of orthogonal search directions.
The orthogonal set is derived from Gram-Schmidt conjugations of the gradients.
In order to optimise any continuous nonlinear function, the Polak-Ribiere formula,

\begin{equation}\label{eq:Polak_Ribiere}
    \bm{\beta_{k}} = \max{ \left( \frac{\bm{g}^T_{k}(\bm{g}_{k} - \bm{g}_{k-1})}{\bm{g}^T_{k-1} \bm{g}_{k-1}}, 0 \right) },
\end{equation}
\noindent
is used to determine the optimal Gram-Schmidt constant $\beta$,
where $\bm{g}$ is the current gradient vector and $\bm{g}_{k-1}$ is the gradient vector of the previous iteration.
The $\max$ operation in Equation \eqref{eq:Polak_Ribiere} is an addition to the original Polak-Riebere formula, added to ensure convergence \cite{shewchuk1994introduction}.
The algorithm will restart CGD with "Method of Steepest Descent" if the gradients are no longer decreasing, thus ensuring convergence.
Note that this resets the orthogonal set of search directions, but the algorithm still converges in the order of $O(N)$ iterations, where $N$ is the number of parameters.
Consequently, CGD can be mathematically expressed as follows \cite{shewchuk1994introduction}:


\begin{align}\label{eq:CGD}
    \begin{split}
        \bm{d}_{0}        & = - \bm{g}_{0}                                                    \\
        \alpha_{i}        & = \min_{\alpha} \left\{  J\left(\bm{\theta} + \alpha \bm{d}_{i} \right) \right\} \\
        \bm{\theta}_{i+1} & = \bm{\theta}_{i} + \alpha_{i} \bm{d}_{i}                         \\
        \bm{d}_{i+1}      & = -\bm{g}_{i+1} + \beta_{i+1} \bm{d}_{i}.                          \\
    \end{split}
\end{align}

In Equation \eqref{eq:CGD}, $\bm{d}_{i}$ is the search direction, $\bm{g}_{i}$ is the gradient, and $\bm{\theta}_{i}$ is the current set of parameters.
As already mentioned, $J(\bm{\theta})$ is the cost function, $\alpha_{i}$ is the optimal step size, which is determined by a line search, and $\beta_{i+1}$ is the Gram-Schmidt constant derived from the Polak-Ribiere formula \eqref{eq:Polak_Ribiere}.


%Consider illustrating the Hessian and/or the Jacobian

% Some equations regarding CGD... Do not now it that well.

\section{Automatic Differentiation}\label{sec:theory_AD}
\noindent
Automatic differentiation (AD) is an algorithmic technique for computing the analytical gradients of a function using computational graphs and the chain rule.
It is important to contrast AD from numerical differentiation using finite differences, which comes with substantial numerical error.
Moreover, AD should not be confused with symbolic differentiation, which is a method for calculating the full symbolic expression for the gradient, like one would do by hand \cite{baydin2018automatic}.
The superior routine for implementing AD is the "reverse mode", which consists of a forward pass and a backward pass.
The forward pass is a function evaluation, where a computational graph, like Figure \ref{} illustrates, is constructed.

% \begin{figure}
%     \centering
%     \includegraphics{}
%     \caption{A computational graph for the function $f(x,y) = $.}
%     \label{fig:AD_comp_graph}
% \end{figure}

In regard to computer science, the computational graph can easily be constructed from an object-oriented operator overloading approach.
To elaborate, the AD object inserts the operators from the function to a computational graph.
Moreover, the rules of differentiation for the operators are pre-implemented.
Therefore, it is required that the evaluated function only consists of operators that are supported by the AD object.
With the function evaluation completed, the backward pass is initiated. Using the chain rule,

\begin{equation}\label{eq:chain_rule}
    \frac{\partial w}{\partial x} = \frac{\partial w}{\partial y} \frac{\partial y}{\partial x},
\end{equation}
the gradients with respect to the input variables are calculated given the output value.
Equation \eqref{eq:chain_rule} shows how the gradient of a functional is simplified to a product series of simple gradient expressions.
Here, $w(y(x))$ is a functional depending on the function $y(x)$, which is again a function of the input variable $x$.
The backpropagation algorithm recursively applies the chain rule on the computational graph, eventually ending up with the gradients of the input variables \cite{baydin2018automatic}.

In terms of advantages, AD can be a powerful tool for optimisation if the symbolic expression is difficult to derive, or ends up being an uncondensed expression.
Furthermore, AD is versatile in deep learning tool boxes such as Pytorch and Tensorflow,
because it allows everyone to implement their own custom neural network architectures or optimisation algorithms.
However, AD is not without its disadvantages. As pointed out by Baydin, Pearlmutter, Radul, and Siskind \cite{baydin2018automatic},
AD is not immune to floating point numbers.
In particular, it is worth keeping in mind that the gradients are computed from the programmed function, not necessarily the true analytical function.
For instance, if one works with an approximation of the exponential function, $\exp(x)$, AD will return the gradient with respect to this approximation,
while one would actually want the analytical gradient $\exp(x)$ \cite{baydin2018automatic}. % Consider another cite Jeffrey Mark Siskind and Barak A. Pearlmutter. Perbupation confusion. The gradient of the exponential function $exp(x)$, for instance, may 







